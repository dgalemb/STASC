{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\n",
    "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the “backend engine” of Keras.*\n",
    "The Python Keras API uses TensorFlow™ as it’s default tensor backend engine, however it’s possible to use other backends if desired. At this time, Keras has two backend implementations available:\n",
    "-  TensorFlow is an open-source symbolic tensor manipulation framework developed by Google.\n",
    "-  Theano is an open-source symbolic tensor manipulation framework developed by LISA Lab at Université de Montréal.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que la libraire Keras est bien installée.\n",
    "> Dans le cas contraire vous pouvez installer Keras via le [gestionnaire d'environnement d'Anaconda](https://www.logicalfeed.com/posts/1224/install-tensorflow-keras-and-theano-using-anaconda-for-deep-learning), en [ligne de commandes conda](https://anaconda.org/conda-forge/keras) ou encore avec `pip install Keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons illustrer les fonctionnalitées de [Keras](https://keras.io/getting_started/intro_to_keras_for_engineers/) sur la base de données des [handwritten digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) de sckit-learn.\n",
    "\n",
    "Il s'agit d'un problème de classification à 10 classes (10 digits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEnCAYAAACzJRZYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVkklEQVR4nO3df0yV5/3/8deRI1AoYEVBGYhMzRQRpUBXRCddEcusqVunW6OVzrWRFi3ULNHOdKVdKt0fa+jSlRTjQKcVuk6tplWLs2CXhhaptFYbq4IFfyDTKaAzWOD+/PGNfMcU9cLzi+Pzkdx/3Pe5bq73sT2vXOc+93VfNsuyLAHALRrk7gIADCyEBgAjhAYAI4QGACOEBgAjhAYAI4QGACOEBgAjhAYAI4SGlygtLZXNZtPx48eNz83Pz5fNZtPZs2cdVs/Vv9kfXV1deu211/TQQw8pMjJSAQEBmjBhglauXKkLFy44rEb0D6EBj3P58mXl5+crOjpahYWF+uCDD/TUU0+puLhYqampunz5srtLvKPZ3V0A8L/uuusuNTQ0KDQ0tOdYWlqaRo0apXnz5unvf/+7Fi5c6MYK72yMNLxYRUWFHnnkEUVGRsrf319jx47VkiVL+vwa0tTUpJ/97GcKDg5WSEiIFi5cqH/961/XtCsvL1dKSooCAwN19913a9asWdq/f7/D6vbx8ekVGFfdd999PXXCfQgNL3bs2DGlpKSoqKhIH374oX73u9/p008/1bRp0/Tdd99d0/6nP/2pxo4dq3fffVf5+fnaunWrZs2a1avt6tWr9dhjjyk2NlbvvPOO/vrXv6q9vV3Tp0/XoUOHbljP1esupaWl/Xo/e/bskSRNnDixX+fDQSx4hZKSEkuS1dDQcN3Xu7u7re+++8769ttvLUnWe++91/Paiy++aEmynnvuuV7nbNy40ZJkbdiwwbIsy2psbLTsdru1bNmyXu3a29utESNGWPPnz7/mb/63devWWT4+Pta6deuM39+JEyes8PBwKykpyerq6jI+H47DSMOLtbS0KDs7W1FRUbLb7Ro8eLCio6MlSV9//fU17RcsWNBrf/78+bLb7froo48kSbt27VJnZ6cWLVqkzs7Ons3f318zZsxQZWXlDeu5et6iRYuM3se///1v/eQnP5FlWSovL9egQfxv605cCPVS3d3dysjI0KlTp/TCCy9o0qRJCgwMVHd3t+6///7r/gIxYsSIXvt2u12hoaE6d+6cJOnMmTOSpOTk5Ov26YwP8/nz5zVz5kydPHlSe/bs0fe//32H9wEzhIaX+uqrr/TFF1+otLRUWVlZPcePHj3a5znNzc363ve+17Pf2dmpc+fO9VyUHDZsmCTp3Xff7RmxONP58+eVnp6uhoYG/eMf/1B8fLzT+8TNERpe6uqNVX5+fr2Ov/XWW32es3HjRiUmJvbsv/POO+rs7FRaWpokadasWbLb7Tp27JgeffRRxxf9X64GRn19vSoqKpSQkODU/nDrCA0vNX78eI0ZM0YrV66UZVkaOnSotm/froqKij7P2bx5s+x2u2bOnKmDBw/qhRde0OTJkzV//nxJ0ujRo/Xyyy9r1apVqq+v10MPPaR77rlHZ86c0WeffabAwEC99NJLff799evXa/HixfrLX/5yw+saly9f7vkZt7CwUJ2dnaquru55ffjw4RozZkw//lXgCISGlxo8eLC2b9+u3NxcLVmyRHa7Xenp6dq9e7dGjRp13XM2b96s/Px8FRUVyWazac6cOSosLJSvr29Pm+eff16xsbF6/fXXtWnTJnV0dGjEiBFKTk5Wdnb2DWvq7u5WV1eXuru7b9juzJkzqqmpkSTl5uZe83pWVla/f7bF7bNZFk8jB3Dr+O0KgBFCA4ARQgOAEUIDgBFCA4ARQgOAEZffp9Hd3a1Tp04pKCio34+DA+B4lmWpvb1dERERN5xH5PLQOHXqlKKiolzdLYBb1NTUpMjIyD5fd3loBAUFSfp/hQUHB7u6e6/3/vvvu7S/FStWuLQ/SX3e0epMRUVFLu/TFZMC/1tbW5uioqJ6PqN9cXloXP1KEhwcTGg4QUBAgEv7c8ezLex2189+uNkHyRnc9fm42WUDLoQCMEJoADBCaAAwQmgAMEJoADBCaAAwQmgAMNKv0HjzzTcVExMjf39/JSYm6uOPP3Z0XQA8lHFolJeXKy8vT6tWrdL+/fs1ffp0ZWZmqrGx0Rn1AfAwxqHx2muv6de//rWefPJJTZgwQYWFhYqKinLLbbYAXM8oNK5cuaLa2lplZGT0Op6RkaFPPvnEoYUB8ExGN/GfPXtWXV1dCg8P73U8PDxczc3N1z2no6NDHR0dPfttbW39KBOAp+jXhdD/ndBiWVafk1wKCgoUEhLSszEtHhjYjEJj2LBh8vHxuWZU0dLScs3o46rnn39era2tPVtTU1P/qwXgdkah4evrq8TExGuW9quoqNDUqVOve46fn1/PNHimwwMDn/GDCZYvX67HH39cSUlJSklJUXFxsRobG2+6JB8A72AcGr/4xS907tw5vfzyyzp9+rTi4uL0wQcfuPwpQwDco1+PQHrmmWf0zDPPOLoWAAMAc08AGCE0ABghNAAYITQAGCE0ABghNAAYITQAGCE0ABhx/fp2d5DCwkKX95mfn+/S/vLy8lzanySVlpa6vM/jx4+7vM/Ro0e7vM9bwUgDgBFCA4ARQgOAEUIDgBFCA4ARQgOAEUIDgBFCA4ARQgOAEePQ2Lt3r+bMmaOIiAjZbDZt3brVCWUB8FTGoXHp0iVNnjxZb7zxhjPqAeDhjOeeZGZmKjMz0xm1ABgAuKYBwIjTZ7myADTgXZw+0mABaMC7OD00WAAa8C5O/3ri5+cnPz8/Z3cDwEWMQ+PixYs6evRoz35DQ4Pq6uo0dOhQjRo1yqHFAfA8xqGxb98+PfDAAz37y5cvlyRlZWW55TFsAFzLODTS0tJkWZYzagEwAHCfBgAjhAYAI4QGACOEBgAjhAYAI4QGACOEBgAjhAYAIywA7URDhgxxeZ91dXUu7e/ChQsu7U+SWx4xOWXKFJf36akYaQAwQmgAMEJoADBCaAAwQmgAMEJoADBCaAAwQmgAMEJoADBiFBoFBQVKTk5WUFCQwsLCNHfuXB0+fNhZtQHwQEahUVVVpZycHFVXV6uiokKdnZ3KyMjQpUuXnFUfAA9jNPdk586dvfZLSkoUFham2tpa/ehHP3JoYQA8021NWGttbZUkDR06tM82rOUKeJd+Xwi1LEvLly/XtGnTFBcX12c71nIFvEu/Q2Pp0qX68ssvtWnTphu2Yy1XwLv06+vJsmXLtG3bNu3du1eRkZE3bMtaroB3MQoNy7K0bNkybdmyRZWVlYqJiXFWXQA8lFFo5OTk6O2339Z7772noKAgNTc3S5JCQkJ01113OaVAAJ7F6JpGUVGRWltblZaWppEjR/Zs5eXlzqoPgIcx/noC4M7G3BMARggNAEYIDQBGCA0ARggNAEYIDQBGCA0ARggNAEZYANqJnnjiCZf36eoFmefOnevS/iSpsLDQ5X26YzFvT8VIA4ARQgOAEUIDgBFCA4ARQgOAEUIDgBFCA4ARQgOAEUIDgBHjZ4TGx8crODhYwcHBSklJ0Y4dO5xVGwAPZBQakZGRevXVV7Vv3z7t27dPP/7xj/XII4/o4MGDzqoPgIcxmnsyZ86cXvuvvPKKioqKVF1drYkTJzq0MACeqd8T1rq6uvS3v/1Nly5dUkpKSp/tWAAa8C7GF0IPHDigu+++W35+fsrOztaWLVsUGxvbZ3sWgAa8i3Fo/OAHP1BdXZ2qq6v19NNPKysrS4cOHeqzPQtAA97F+OuJr6+vxo4dK0lKSkpSTU2NXn/9db311lvXbc8C0IB3ue37NCzL6nXNAoB3Mxpp/Pa3v1VmZqaioqLU3t6usrIyVVZWaufOnc6qD4CHMQqNM2fO6PHHH9fp06cVEhKi+Ph47dy5UzNnznRWfQA8jFForF271ll1ABggmHsCwAihAcAIoQHACKEBwAihAcAIoQHACKEBwAhruTqRq9dVlVy/fmxaWppL+3NXn/j/GGkAMEJoADBCaAAwQmgAMEJoADBCaAAwQmgAMEJoADBCaAAwQmgAMHJboVFQUCCbzaa8vDwHlQPA0/U7NGpqalRcXKz4+HhH1gPAw/UrNC5evKgFCxZozZo1uueeexxdEwAP1q/QyMnJ0ezZs5Wenn7Tth0dHWpra+u1ARi4jKfGl5WV6fPPP1dNTc0ttS8oKNBLL71kXBgAz2Q00mhqalJubq42bNggf3//WzqHBaAB72I00qitrVVLS4sSExN7jnV1dWnv3r1644031NHRIR8fn17nsAA04F2MQuPBBx/UgQMHeh371a9+pfHjx2vFihXXBAYA72MUGkFBQYqLi+t1LDAwUKGhodccB+CduCMUgJHbfrBwZWWlA8oAMFAw0gBghNAAYITQAGCE0ABghNAAYITQAGCE0ABg5I5ZADo/P9/lfbpjdu/kyZNd2t/WrVtd2h/cj5EGACOEBgAjhAYAI4QGACOEBgAjhAYAI4QGACOEBgAjhAYAI4QGACNGoZGfny+bzdZrGzFihLNqA+CBjOeeTJw4Ubt37+7ZZ9kC4M5iHBp2u53RBXAHM76mceTIEUVERCgmJka//OUvVV9ff8P2LAANeBej0PjhD3+o9evXa9euXVqzZo2am5s1depUnTt3rs9zCgoKFBIS0rNFRUXddtEA3McoNDIzM/Xoo49q0qRJSk9P1/vvvy9JWrduXZ/nsAA04F1u6yE8gYGBmjRpko4cOdJnGxaABrzLbd2n0dHRoa+//lojR450VD0APJxRaPzmN79RVVWVGhoa9Omnn+rnP/+52tralJWV5az6AHgYo68nJ06c0GOPPaazZ89q+PDhuv/++1VdXa3o6Ghn1QfAwxiFRllZmbPqADBAMPcEgBFCA4ARQgOAEUIDgBFCA4ARQgOAEUIDgJE7ZgHohIQEl/fpjpvevvjiC5f2N3fuXJf2J0mFhYUu73P06NEu79NTMdIAYITQAGCE0ABghNAAYITQAGCE0ABghNAAYITQAGCE0ABghNAAYMQ4NE6ePKmFCxcqNDRUAQEBmjJlimpra51RGwAPZDT35Pz580pNTdUDDzygHTt2KCwsTMeOHdOQIUOcVB4AT2MUGn/4wx8UFRWlkpKSnmNM5AHuLEZfT7Zt26akpCTNmzdPYWFhSkhI0Jo1a254DgtAA97FKDTq6+tVVFSkcePGadeuXcrOztazzz6r9evX93kOC0AD3sUoNLq7u3Xvvfdq9erVSkhI0JIlS/TUU0+pqKioz3NYABrwLkahMXLkSMXGxvY6NmHCBDU2NvZ5jp+fn4KDg3ttAAYuo9BITU3V4cOHex375ptvWJYRuIMYhcZzzz2n6upqrV69WkePHtXbb7+t4uJi5eTkOKs+AB7GKDSSk5O1ZcsWbdq0SXFxcfr973+vwsJCLViwwFn1AfAwxg8Wfvjhh/Xwww87oxYAAwBzTwAYITQAGCE0ABghNAAYITQAGCE0ABghNAAYsVmWZbmyw7a2NoWEhKi1tZV5KE5QWlrq1f1J0oULF1zepzve55QpU1za361+NhlpADBCaAAwQmgAMEJoADBCaAAwQmgAMEJoADBCaAAwQmgAMGIUGqNHj5bNZrtm4xmhwJ3D6HF/NTU16urq6tn/6quvNHPmTM2bN8/hhQHwTEahMXz48F77r776qsaMGaMZM2Y4tCgAnqvf1zSuXLmiDRs2aPHixbLZbI6sCYAHM34a+VVbt27VhQsX9MQTT9ywXUdHhzo6Onr2WQAaGNj6PdJYu3atMjMzFRERccN2LAANeJd+hca3336r3bt368knn7xpWxaABrxLv76elJSUKCwsTLNnz75pWz8/P/n5+fWnGwAeyHik0d3drZKSEmVlZclu7/clEQADlHFo7N69W42NjVq8eLEz6gHg4YyHChkZGXLxY0UBeBDmngAwQmgAMEJoADBCaAAwQmgAMEJoADBCaAAw4vJbOq/e48FsV+e4fPmyS/vr7Ox0aX+Sej0IylUuXrzo8j5d/Rm52t/N7sNy+QLQJ06cYKYr4MGampoUGRnZ5+suD43u7m6dOnVKQUFBRg/vaWtrU1RUlJqamrx6tXnep/cYaO/Rsiy1t7crIiJCgwb1feXC5V9PBg0adMMUu5ng4OAB8R/gdvE+vcdAeo8hISE3bcOFUABGCA0ARgZMaPj5+enFF1/0+gf68D69h7e+R5dfCAUwsA2YkQYAz0BoADBCaAAwQmgAMDIgQuPNN99UTEyM/P39lZiYqI8//tjdJTlUQUGBkpOTFRQUpLCwMM2dO1eHDx92d1lOV1BQIJvNpry8PHeX4nAnT57UwoULFRoaqoCAAE2ZMkW1tbXuLsshPD40ysvLlZeXp1WrVmn//v2aPn26MjMz1djY6O7SHKaqqko5OTmqrq5WRUWFOjs7lZGRoUuXLrm7NKepqalRcXGx4uPj3V2Kw50/f16pqakaPHiwduzYoUOHDumPf/yjhgwZ4u7SHMPycPfdd5+VnZ3d69j48eOtlStXuqki52tpabEkWVVVVe4uxSna29utcePGWRUVFdaMGTOs3Nxcd5fkUCtWrLCmTZvm7jKcxqNHGleuXFFtba0yMjJ6Hc/IyNAnn3zipqqcr7W1VZI0dOhQN1fiHDk5OZo9e7bS09PdXYpTbNu2TUlJSZo3b57CwsKUkJCgNWvWuLssh/Ho0Dh79qy6uroUHh7e63h4eLiam5vdVJVzWZal5cuXa9q0aYqLi3N3OQ5XVlamzz//XAUFBe4uxWnq6+tVVFSkcePGadeuXcrOztazzz6r9evXu7s0hxgQ6yr+7xR6y7KMptUPJEuXLtWXX36pf/7zn+4uxeGampqUm5urDz/8UP7+/u4ux2m6u7uVlJSk1atXS5ISEhJ08OBBFRUVadGiRW6u7vZ59Ehj2LBh8vHxuWZU0dLScs3owxssW7ZM27Zt00cffXRbjw/wVLW1tWppaVFiYqLsdrvsdruqqqr0pz/9SXa73S1P5HKGkSNHKjY2ttexCRMmeM3Fe48ODV9fXyUmJqqioqLX8YqKCk2dOtVNVTmeZVlaunSpNm/erD179igmJsbdJTnFgw8+qAMHDqiurq5nS0pK0oIFC1RXVycfHx93l+gQqamp1/xk/s033yg6OtpNFTmYmy/E3lRZWZk1ePBga+3atdahQ4esvLw8KzAw0Dp+/Li7S3OYp59+2goJCbEqKyut06dP92z/+c9/3F2a03njryefffaZZbfbrVdeecU6cuSItXHjRisgIMDasGGDu0tzCI8PDcuyrD//+c9WdHS05evra917771e91OkpOtuJSUl7i7N6bwxNCzLsrZv327FxcVZfn5+1vjx463i4mJ3l+QwTI0HYMSjr2kA8DyEBgAjhAYAI4QGACOEBgAjhAYAI4QGACOEBgAjhAYAI4QGACOEBgAjhAYAI/8HonsF91t4FnUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 50\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Normaliser les données avec la fonction `preprocessing.scale()` et séparer celles-ci en deux échantillons d'apprentissage et de test. Afficher la dimension des deux jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data_nor = preprocessing.scale(digits.data)\n",
    "\n",
    "X_train_nor, X_test_nor, y_train, y_test = #### TO DO ####  test_size=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dimensions des deux jeux de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_nor.shape, y_train.shape)\n",
    "print(X_test_nor.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir et ajuster un modèle à propagation avant (Feed Forward) avec Keras.\n",
    "\n",
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable de classe (chiffre correspondant) indique le numéro de la catégorie : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Keras, pour construire un réseau de neurones adapté à ce problème de la classification multiclasses, il faut préalablement transformer la variable de classe sous la forme \"one hot encoding\" i.e. transformer la variable de classe en un vecteur indiquant l'appartenance à chacune des classes sous forme d'indicatrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition de l'architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant construire un premier réseau à propagation avant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les fonctionnalités qui permettent de définir, de façon successive, les différentes couches :\n",
    "- Dense : permet de définir les pré-activations, c'est dans cette couche que se situent les poids et les termes de biais.\n",
    "- Activation :  permet de définir les fonctions d'activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dimensions du modèle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_train_nor.shape[1]  # dimension en entrée\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 128 # dimension de la couche cachée\n",
    "K = 10  # nombre de classes en sortie (autant que de chiffres ici)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant définir l'architecture du réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(K))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de résumer l'architecture avec la méthode `summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de paramètres est indiqué dans la dernière colonne, notez que seules les couches `dense` contiennent ici des paramètres à ajuster.\n",
    "\n",
    "Les dimensions en sortie de chaque couche sont de la forme `(None, ...)`. Lorsque le réseau sera ajusté sur un échantillon, la première dimension indiquera la taille de l'échantillon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visualiser (dans un fichier .png) l'architecture ainsi. Si vous êtes confronté à des problèmes d'installation (librairies `pydot`, `graphviz` ...), ne perdez pas trop de temps sur cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poids du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu (les poids) de chaque couche du réseau est accessible dans la liste `model.layers`. Par exemple pour la premiere couche :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau contenant les valeurs des poids et les termes de biais de cette première couche sont accessibles avec la méthode `get_weights()`. La méthode renvoie en fait une liste contenant\n",
    "- le tableau numpy des poids,\n",
    "- le vecteur des termes de biais.\n",
    "\n",
    "> Vérifier que le tableau des poids ainsi que les termes de biais possèdent les dimensions attendues compte tenu de ce que nous avons vu dans le `model.summary()` plus haut. Retrouvez le nombre de paramètres affichés dans le `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bien que le réseau n'ait pas encore été ajusté sur des données, vérifier que la définition de l'architecture implique une intialisation immédiate des poids du réseau (i.e. les poids prennent des valeurs). \n",
    "> Vérifier aussi que les biais sont initialisés à zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage : ajustement des poids du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La phase d'apprentissage consiste à estimer le vecteur $\\theta$ (qui contient tous les paramètres du réseau : poids et biais) en cherchant à minimiser en $\\theta$ le risque empirique (ici sans pénalité) \n",
    "$$ \\theta \\mapsto \\frac 1{n} \\sum_{i=1}^n  \\ell( f(X_i , \\theta),Y_i) $$\n",
    "où $f(\\cdot , \\theta)$ est la fonction qui correspond au réseau de neurones et où $\\ell$ est la perte choisie. Pour le problème de classification multiclasses qui nous intéresse ici il s'agit de la cross-entropy.\n",
    "\n",
    "On utilise pour cela une méthode d'optimisation de type descente de gradient stochastique, appliquée au risque empirique pénalisé. Par exemple pour `SGD`, chaque itération effectue un pas de gradient, en calculant ce gradient sur un sous-échantillon des données (échantillon batch):\n",
    "$$ \\theta_b  =   \\theta_{b-1}  - \\varepsilon   \\frac 1{|B|} \\sum_{i \\in B}  \\nabla_\\theta \\ell( f(X_i , \\theta_{b-1}),Y_i)   \n",
    " $$\n",
    "où $\\varepsilon$ est le taux d'apprentissage (learning rate).\n",
    "\n",
    "Le module `optimizers` de Keras permet de choisir la méthode d'optimisation. On utilise ci-dessous l'optimiseur `SGD`, avec l'argument `lr` pour régler le taux d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant ajuster sur les données digits un réseau de neurones à une couche cachée.\n",
    "\n",
    "L'ajustement est effectué en deux temps : compilation puis ajustement sur les données.\n",
    "\n",
    "On spécifie les éléments nécessaires à l'ajustement  avec la méthode  `compile` :\n",
    "- optimiseur,\n",
    "- perte considérée pour la back propagation (dans l'algo de descente de gradient)\n",
    "- \"métriques\" à considérer pour évaluer l'erreur. On pourrait bien sûr évaluer l'erreur uniquement pour la cross-entropy, mais l'erreur pour la perte sèche $\\ell(y,y') = 1_{y \\neq y'}$ est plus facile à interpréter, on choisit donc de suivre  aussi la métrique \"accuracy\" (précision = taux de bien classé) le long de la trajectoire de descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les arguments de la méthode `.fit` ci-dessous:\n",
    "- epochs = nombre de passages sur l'ensemble de l'échantillon  ;\n",
    "- batch_size = taille du sous-échantillon extrait (batch) pour le calcul du gradient à chaque itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_nor, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chaque ligne correspond à un passage sur l'ensemble des données (= un epoch). Nous avons choisi un batch size de 32 observations. Combien de pas de gradients sont donc effectués par epoch pour ce choix de paramètres ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La perte cross entropy (ici utilisé pour désigner le risque empirique pour la cross entropy) et la précision (accuracy) s'améliorent-elles le long de la trajectoire de descente de gradient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les poids sont enregistrés dans `model.layers`. \n",
    "\n",
    "> Les biais sont ils toujours à zero ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En comparant les valeurs de la perte et de l'accuracy, vérifier (sur les sorties) que si l'on effectue l'ajustement une seconde fois, l'état initial pour le second essai correspond à l'état du réseau précédemment ajusté."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Essayer de diminuer encore la perte en augmentant le nombre d'épochs, en diminuant le taux d'apprentissage (par exemple 0.001).   \n",
    "> Remarque : si vous souhaitez réinitialiser les poids, il vous faut \"réinstancier\" le modèle en le redéfinissant totalement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(K))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Que dire de la convergence pour une grande valeur du taux d'apprentissage (par exemple `lr=2`) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=2) \n",
    "\n",
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Essayer aussi d'utiliser une méthode `Momentum-SGD` (argument `momentum= True`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions et erreur de généralisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Effectuer des prédictions de votre réseau sur l'échantillon test avec la méthode `.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "\n",
    "\n",
    "y_predicted = ### TO DO ###\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour plusieurs élements de l'échantillon test, renvoyer les probabilités conditionnelles prédites par le réseau (en sortie du Softmax) à l'aide de la méthode `.model.predict()`. La probabilité de la classe choisie est-elle générallement bien supérieure aux probabilités des autres classes ?\n",
    "Calculer le taux de bien classé sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 2\n",
    "plt.imshow(X_test_nor[sample_idx].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest');\n",
    "plt.title(\"predicted label: %d\\n true label: %d\"\n",
    "              % (y_predicted[sample_idx], y_test[sample_idx]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = ### TO DO ###\n",
    "probabilities[sample_idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(X_test_nor[i].reshape(8, 8),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"predicted label: %d\\n true label: %d\"\n",
    "              % (y_predicted[i], y_test[i]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que la perte empirique pour la cross-entropy vaut\n",
    "\\begin{eqnarray*}\n",
    "\\hat{ \\mathcal R}_n (\\theta)  &=&  \\frac 1n  \\sum_{i=1}^n  \\ell( Y_i,  f(X_i , \\theta)) \\\\ \n",
    "& =&   -    \\frac 1n \\sum_{i=1}^n  \\sum_{k = 1}^K  \\mathbb{1}_{Y_i =  k}  \\log (f(X_i , \\theta)_k)  \n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculer (\"à la main\") le risque empirique (pour la cross entropy) sur l'échantillon test. Le risque était-il plus faible sur l'échantillon d'apprentissage  ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plus simplement, on peut utiliser la méthode `.evaluate(x,y)`  pour retrouver la perte empirique et l'accuracy. Ce calcul correspond-il à une passe de propagation avant ou bien à un pas de back-propagation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact de l'initialisation\n",
    "\n",
    "Nous allons maintenant étudier l'impact de l'initialisation de la méthode SGD sur la convergence de ce dernier.\n",
    "\n",
    "Dans Keras, par défaut les poids sont initialisés comme suit par l'initialiseur `glorot_uniform` :\n",
    "\n",
    "- chaque poids est tiré uniformément dans $[-scale, scale]$\n",
    "- scale est choisi de l'ordre de $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "Nous allons étudier l'impact de l'initialisation en considérant des initialiseurs Gaussiens en testant plusieurs valeurs pour la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "mon_init1 = initializers.RandomNormal(mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Définir un réseau avec une couche Softmax en sortie et deux couches cachées \"tanh\", avec respectivement H neurones et K neurones. (On utilise ici des activations \"tanh\" pour que le problème soit un peu plus difficile à résoudre pour le réseau ...)\n",
    "> Passer l'initialiseur en argument de `Dense()`, ce qui donne par exemple pour la première couche :   \n",
    "`model.add(Dense(H,input_dim=N,kernel_initializer=mon_init1))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ajuster le réseau et afficher l'évolution de la perte (i.e. risque empirique) sur les données d'apprentissage le long de la trajectoire des epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(### TO DO ###) \n",
    "\n",
    "history1 = model.fit(X_train_nor, Y_train,epochs=20,\n",
    "                     batch_size=32,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut afficher l'évolution de la perte sur l'apprentissage comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['loss'],label = \"stddev=0.05\")\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Training loss')\n",
    "#plt.ylim(0, 6)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Faire de même avec les initialiseurs suivants :    \n",
    "> `mon_init2 = initializers.RandomNormal(mean=0.0,stddev=10)`   \n",
    "> `mon_init3 = initializers.RandomNormal(mean=0.0,stddev=0.001)` \\\n",
    "> `mon_init4 = initializers.glorot_uniform()`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparer les performance des réseaux ajustés avec ces initialiseurs en représentant les \"trajectoires d'apprentissage\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- Initialisation trop proche de zero : gradients trop faibles et SGD peine à sortir de cette zone ...\n",
    "- Initialisation avec des amplitudes trop élevées : les couches intermédiaires ont des gradients qui s'annulent et difficile là encore de sortir de cette zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'argument ` validation_split=0.1` dans la méthode `.fit()` permet d'évaluer la perte sur un échantillon de validation. \n",
    "\n",
    "> Vérifier graphiquement qu'après un certain nombre d'itérations (quelques centaines epochs ici), le risque et la précision sur l'échantillon de validation  finissent par se dégrader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux de neurones, même sans parler des architectures \"profondes\", nécessitent d'ajuster un très grand nombre de paramètres. Il est donc naturel de devoir contrôler la complexité statistique de ces prédicteurs pour en obtenir les meilleurs erreurs de généralisation. \n",
    "\n",
    "A chaque pas de la descente de gradient l'ensemble des poids et des biais du réseau est mis à jour. A chaque pas de la descente correspond donc un prédicteur, et au final on dispose donc d'une suite de prédicteurs. Le prédicteur final (du dernier pas de gradient) est certes celui qui minimise le risque empirique (appelée ici perte) mais cela ne signifie évidemment pas que ce prédicteur a les meilleurs performances sur l'échantillon de validation (ou de test).\n",
    "\n",
    "\n",
    "La méthode classiquement utilisée pour choisir le nombre de pas de gradients est celle du \"early stopping\".  Le principe consiste tout simplement à stopper la descente de gradient stochastique lorsque l'erreur de validation remonte. L'argument `monitor='val_loss'` de la fonction `EarlyStopping` indique que c'est l'erreur de validation calculée pour la perte (ici cross entropy) qui sera surveillée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le long d'une trajectoire de descente de gradient, il est possible que la perte remonte ponctuellement sur une itération, avant de redescendre aussitôt. Il est donc préférable de ne prendre la décision d'arrêter la descente de gradient que si cette remontée se confirme sur quelques itérations successives. L'argument `patience=5` impose à la descente de gradient de patienter 5 itérations après une remontée de la perte, avant de prendre la décision d'arrêter  ou non l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(H,input_dim=N, kernel_initializer=mon_init4))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K,kernel_initializer=mon_init4))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K,kernel_initializer=mon_init4))\n",
    "model.add(Activation(\"softmax\"))\n",
    "sgd = SGD(lr=0.1) \n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit(X_train_nor, Y_train,epochs=500,\n",
    "          batch_size=32,verbose=0,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],label =\"train\")\n",
    "plt.plot(history.history['val_loss'],label =\"valid\")\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Donner la prédiction du dernier modèle ajusté (par early stopping) sur l'échantillon test `X_test_nor`. Calculer le risque estimé sur les données de test en utilisant la méthode `.evaluate()`. Calculer enfin le taux de mauvais classement sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Données MNIST\n",
    "\n",
    "Charger les données [handwritten digits MNIST](http://yann.lecun.com/exdb/mnist/) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist # subroutines for fetching the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  \n",
    "\n",
    "height, width, depth = 28, 28, 1 #  28x28 and 1 canal pour le niveau de gris\n",
    "num_classes = 10\n",
    "\n",
    "num_train = 60000 \n",
    "num_test = 10000  \n",
    "\n",
    "X_train = X_train.reshape(num_train, height * width)  # vecteur \n",
    "X_test = X_test.reshape(num_test, height * width) # vecteur \n",
    "\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train_nor = X_train / 255 # normalisation dans  [0, 1]  \n",
    "X_test_nor = X_test / 255 # normalisation dans  [0, 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Définir et ajuster un réseau de neurones à deux couches cachées. \n",
    "> On pourra dans un premier temps choisir les paramètres ci-dessous. \n",
    "> On pourra aussi utiliser l'optimiseur adagrad :    \n",
    "> `adag = keras.optimizers.Adagrad(lr=0.01)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  \n",
    "num_epochs = 40  \n",
    "hidden_size = 512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En reproduisant les codes proposés sur ce [blog](https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/), essayer d'améliorer les performances de votre réseau en choisissant mieux les paramètres par validation croisée à l'aide de la fonction `GridSearchCV` de sckit-learn. Attention, cette validation croisée s'avère très couteuse en temps de calcul..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
