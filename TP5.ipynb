{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network avec Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "# si nécessaire : conda install -c conda-forge opencv   \n",
    "# ou sinon : https://pypi.org/project/opencv-python/\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les librairies nécessaires de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation et préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger les données depuis :\n",
    "\n",
    "https://box.ec-nantes.fr:443/index.php/s/2NxopNZS3FTRBcr\n",
    "\n",
    "Adresse du dossier où sont entreposées les données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"data_animals\"\n",
    "data_path = \"/Users/dgalembeck/Documents/Coding/Cours/STASC/Data/data_animals\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `os.listdir()` permet de lister le contenu du dossier `data_animals` (un repertoire par classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'cats', 'Humans', 'horses']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "data_dir_list = os.listdir(data_path)\n",
    "print(data_dir_list)\n",
    "num_classes = len(data_dir_list) \n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les images ne sont pas au même format (nb de pixels).\n",
    "Le réseau CNN impose que toutes les données aient la même dimension. Il nous faudra  transformer les images pour qu'elles soient toutes au même format : 128x 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows=128\n",
    "img_cols=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque les images en entrée du réseau sont en couleur, on utilise 3 canaux(RGB).\n",
    "\n",
    "Ici, pour simplifier, nous allons préalablement transformer les images en niveaux de gris et de ce fait nous n'utiliserons qu'un seul canal en entrée du réseau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channel=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compléter le script ci-dessous pour importer les images en niveaux de gris et sous la forme de tableaux 128x128, dans la liste img_data_list.\n",
    "*  `my_img = cv2.imread(\"file\")`  : lecture d'un fichier image\n",
    "*  `cv2.cvtColor(my_img, cv2.COLOR_BGR2GRAY)` : convertit le fichier image en niveaux de gris\n",
    "*  `cv2.resize(input_img,(n,p))` : redimensionne l'image au format n x p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-dogs\n",
      "\n",
      "dog.167.jpg\n",
      "dog.173.jpg\n",
      "dog.36.jpg\n",
      "dog.22.jpg\n",
      "dog.198.jpg\n",
      "dog.199.jpg\n",
      "dog.23.jpg\n",
      "dog.37.jpg\n",
      "dog.172.jpg\n",
      "dog.166.jpg\n",
      "dog.170.jpg\n",
      "dog.164.jpg\n",
      "dog.21.jpg\n",
      "dog.158.jpg\n",
      "dog.35.jpg\n",
      "dog.159.jpg\n",
      "dog.34.jpg\n",
      "dog.20.jpg\n",
      "dog.165.jpg\n",
      "dog.171.jpg\n",
      "dog.149.jpg\n",
      "dog.24.jpg\n",
      "dog.30.jpg\n",
      "dog.175.jpg\n",
      "dog.18.jpg\n",
      "dog.161.jpg\n",
      "dog.160.jpg\n",
      "dog.174.jpg\n",
      "dog.19.jpg\n",
      "dog.31.jpg\n",
      "dog.148.jpg\n",
      "dog.25.jpg\n",
      "dog.202.jpg\n",
      "dog.200.jpg\n",
      "dog.33.jpg\n",
      "dog.27.jpg\n",
      "dog.162.jpg\n",
      "dog.176.jpg\n",
      "dog.189.jpg\n",
      "dog.188.jpg\n",
      "dog.177.jpg\n",
      "dog.163.jpg\n",
      "dog.26.jpg\n",
      "dog.32.jpg\n",
      "dog.201.jpg\n",
      "dog.104.jpg\n",
      "dog.69.jpg\n",
      "dog.110.jpg\n",
      "dog.138.jpg\n",
      "dog.55.jpg\n",
      "dog.1.jpg\n",
      "dog.41.jpg\n",
      "dog.96.jpg\n",
      "dog.82.jpg\n",
      "dog.83.jpg\n",
      "dog.97.jpg\n",
      "dog.40.jpg\n",
      "dog.139.jpg\n",
      "dog.54.jpg\n",
      "dog.111.jpg\n",
      "dog.105.jpg\n",
      "dog.68.jpg\n",
      "dog.113.jpg\n",
      "dog.107.jpg\n",
      "dog.2.jpg\n",
      "dog.42.jpg\n",
      "dog.56.jpg\n",
      "dog.81.jpg\n",
      "dog.95.jpg\n",
      "dog.94.jpg\n",
      "dog.80.jpg\n",
      "dog.57.jpg\n",
      "dog.43.jpg\n",
      "dog.3.jpg\n",
      "dog.106.jpg\n",
      "dog.112.jpg\n",
      "dog.47.jpg\n",
      "dog.7.jpg\n",
      "dog.53.jpg\n",
      "dog.116.jpg\n",
      "dog.102.jpg\n",
      "dog.84.jpg\n",
      "dog.90.jpg\n",
      "dog.91.jpg\n",
      "dog.85.jpg\n",
      "dog.103.jpg\n",
      "dog.117.jpg\n",
      "dog.52.jpg\n",
      "dog.6.jpg\n",
      "dog.46.jpg\n",
      "dog.50.jpg\n",
      "dog.129.jpg\n",
      "dog.44.jpg\n",
      "dog.4.jpg\n",
      "dog.101.jpg\n",
      "dog.115.jpg\n",
      "dog.78.jpg\n",
      "dog.93.jpg\n",
      "dog.87.jpg\n",
      "dog.86.jpg\n",
      "dog.92.jpg\n",
      "dog.114.jpg\n",
      "dog.79.jpg\n",
      "dog.100.jpg\n",
      "dog.128.jpg\n",
      "dog.5.jpg\n",
      "dog.45.jpg\n",
      "dog.51.jpg\n",
      "dog.125.jpg\n",
      "dog.8.jpg\n",
      "dog.48.jpg\n",
      "dog.131.jpg\n",
      "dog.119.jpg\n",
      "dog.74.jpg\n",
      "dog.60.jpg\n",
      "dog.61.jpg\n",
      "dog.118.jpg\n",
      "dog.75.jpg\n",
      "dog.130.jpg\n",
      "dog.124.jpg\n",
      "dog.49.jpg\n",
      "dog.9.jpg\n",
      "dog.132.jpg\n",
      "dog.126.jpg\n",
      "dog.63.jpg\n",
      "dog.77.jpg\n",
      "dog.88.jpg\n",
      "dog.89.jpg\n",
      "dog.76.jpg\n",
      "dog.62.jpg\n",
      "dog.127.jpg\n",
      "dog.133.jpg\n",
      "dog.66.jpg\n",
      "dog.72.jpg\n",
      "dog.137.jpg\n",
      "dog.123.jpg\n",
      "dog.99.jpg\n",
      "dog.98.jpg\n",
      "dog.122.jpg\n",
      "dog.136.jpg\n",
      "dog.73.jpg\n",
      "dog.67.jpg\n",
      "dog.71.jpg\n",
      "dog.108.jpg\n",
      "dog.65.jpg\n",
      "dog.120.jpg\n",
      "dog.134.jpg\n",
      "dog.59.jpg\n",
      "dog.135.jpg\n",
      "dog.58.jpg\n",
      "dog.121.jpg\n",
      "dog.109.jpg\n",
      "dog.64.jpg\n",
      "dog.70.jpg\n",
      "dog.146.jpg\n",
      "dog.152.jpg\n",
      "dog.17.jpg\n",
      "dog.185.jpg\n",
      "dog.191.jpg\n",
      "dog.190.jpg\n",
      "dog.184.jpg\n",
      "dog.16.jpg\n",
      "dog.153.jpg\n",
      "dog.147.jpg\n",
      "dog.151.jpg\n",
      "dog.145.jpg\n",
      "dog.28.jpg\n",
      "dog.179.jpg\n",
      "dog.14.jpg\n",
      "dog.192.jpg\n",
      "dog.186.jpg\n",
      "dog.187.jpg\n",
      "dog.193.jpg\n",
      "dog.178.jpg\n",
      "dog.15.jpg\n",
      "dog.144.jpg\n",
      "dog.29.jpg\n",
      "dog.150.jpg\n",
      "dog.168.jpg\n",
      "dog.11.jpg\n",
      "dog.154.jpg\n",
      "dog.39.jpg\n",
      "dog.140.jpg\n",
      "dog.197.jpg\n",
      "dog.183.jpg\n",
      "dog.182.jpg\n",
      "dog.196.jpg\n",
      "dog.141.jpg\n",
      "dog.155.jpg\n",
      "dog.38.jpg\n",
      "dog.10.jpg\n",
      "dog.169.jpg\n",
      "dog.12.jpg\n",
      "dog.143.jpg\n",
      "dog.157.jpg\n",
      "dog.180.jpg\n",
      "dog.194.jpg\n",
      "dog.195.jpg\n",
      "dog.181.jpg\n",
      "dog.156.jpg\n",
      "dog.142.jpg\n",
      "dog.13.jpg\n",
      "Loaded the images of dataset-cats\n",
      "\n",
      "cat.6.jpg\n",
      "cat.198.jpg\n",
      "cat.30.jpg\n",
      "cat.24.jpg\n",
      "cat.167.jpg\n",
      "cat.18.jpg\n",
      "cat.173.jpg\n",
      "cat.172.jpg\n",
      "cat.19.jpg\n",
      "cat.166.jpg\n",
      "cat.25.jpg\n",
      "cat.31.jpg\n",
      "cat.199.jpg\n",
      "cat.7.jpg\n",
      "cat.5.jpg\n",
      "cat.27.jpg\n",
      "cat.33.jpg\n",
      "cat.158.jpg\n",
      "cat.170.jpg\n",
      "cat.164.jpg\n",
      "cat.165.jpg\n",
      "cat.171.jpg\n",
      "cat.159.jpg\n",
      "cat.32.jpg\n",
      "cat.26.jpg\n",
      "cat.4.jpg\n",
      "cat.175.jpg\n",
      "cat.161.jpg\n",
      "cat.22.jpg\n",
      "cat.149.jpg\n",
      "cat.36.jpg\n",
      "cat.37.jpg\n",
      "cat.148.jpg\n",
      "cat.23.jpg\n",
      "cat.160.jpg\n",
      "cat.174.jpg\n",
      "cat.202.jpg\n",
      "cat.1.jpg\n",
      "cat.3.jpg\n",
      "cat.200.jpg\n",
      "cat.189.jpg\n",
      "cat.162.jpg\n",
      "cat.176.jpg\n",
      "cat.35.jpg\n",
      "cat.21.jpg\n",
      "cat.20.jpg\n",
      "cat.34.jpg\n",
      "cat.177.jpg\n",
      "cat.163.jpg\n",
      "cat.188.jpg\n",
      "cat.201.jpg\n",
      "cat.2.jpg\n",
      "cat.90.jpg\n",
      "cat.84.jpg\n",
      "cat.53.jpg\n",
      "cat.138.jpg\n",
      "cat.47.jpg\n",
      "cat.104.jpg\n",
      "cat.110.jpg\n",
      "cat.111.jpg\n",
      "cat.105.jpg\n",
      "cat.46.jpg\n",
      "cat.139.jpg\n",
      "cat.52.jpg\n",
      "cat.85.jpg\n",
      "cat.91.jpg\n",
      "cat.87.jpg\n",
      "cat.93.jpg\n",
      "cat.44.jpg\n",
      "cat.50.jpg\n",
      "cat.78.jpg\n",
      "cat.113.jpg\n",
      "cat.107.jpg\n",
      "cat.106.jpg\n",
      "cat.112.jpg\n",
      "cat.79.jpg\n",
      "cat.51.jpg\n",
      "cat.45.jpg\n",
      "cat.92.jpg\n",
      "cat.86.jpg\n",
      "cat.82.jpg\n",
      "cat.96.jpg\n",
      "cat.116.jpg\n",
      "cat.69.jpg\n",
      "cat.102.jpg\n",
      "cat.41.jpg\n",
      "cat.55.jpg\n",
      "cat.54.jpg\n",
      "cat.40.jpg\n",
      "cat.103.jpg\n",
      "cat.68.jpg\n",
      "cat.117.jpg\n",
      "cat.97.jpg\n",
      "cat.83.jpg\n",
      "cat.95.jpg\n",
      "cat.81.jpg\n",
      "cat.101.jpg\n",
      "cat.115.jpg\n",
      "cat.56.jpg\n",
      "cat.42.jpg\n",
      "cat.129.jpg\n",
      "cat.128.jpg\n",
      "cat.43.jpg\n",
      "cat.57.jpg\n",
      "cat.114.jpg\n",
      "cat.100.jpg\n",
      "cat.80.jpg\n",
      "cat.94.jpg\n",
      "cat.99.jpg\n",
      "cat.72.jpg\n",
      "cat.119.jpg\n",
      "cat.66.jpg\n",
      "cat.125.jpg\n",
      "cat.131.jpg\n",
      "cat.130.jpg\n",
      "cat.124.jpg\n",
      "cat.67.jpg\n",
      "cat.118.jpg\n",
      "cat.73.jpg\n",
      "cat.98.jpg\n",
      "cat.65.jpg\n",
      "cat.71.jpg\n",
      "cat.59.jpg\n",
      "cat.132.jpg\n",
      "cat.126.jpg\n",
      "cat.127.jpg\n",
      "cat.133.jpg\n",
      "cat.58.jpg\n",
      "cat.70.jpg\n",
      "cat.64.jpg\n",
      "cat.137.jpg\n",
      "cat.48.jpg\n",
      "cat.123.jpg\n",
      "cat.60.jpg\n",
      "cat.74.jpg\n",
      "cat.75.jpg\n",
      "cat.61.jpg\n",
      "cat.122.jpg\n",
      "cat.49.jpg\n",
      "cat.136.jpg\n",
      "cat.88.jpg\n",
      "cat.120.jpg\n",
      "cat.134.jpg\n",
      "cat.77.jpg\n",
      "cat.63.jpg\n",
      "cat.108.jpg\n",
      "cat.109.jpg\n",
      "cat.62.jpg\n",
      "cat.76.jpg\n",
      "cat.135.jpg\n",
      "cat.121.jpg\n",
      "cat.89.jpg\n",
      "cat.185.jpg\n",
      "cat.191.jpg\n",
      "cat.11.jpg\n",
      "cat.146.jpg\n",
      "cat.39.jpg\n",
      "cat.152.jpg\n",
      "cat.153.jpg\n",
      "cat.38.jpg\n",
      "cat.147.jpg\n",
      "cat.10.jpg\n",
      "cat.190.jpg\n",
      "cat.184.jpg\n",
      "cat.192.jpg\n",
      "cat.186.jpg\n",
      "cat.12.jpg\n",
      "cat.179.jpg\n",
      "cat.151.jpg\n",
      "cat.145.jpg\n",
      "cat.144.jpg\n",
      "cat.150.jpg\n",
      "cat.178.jpg\n",
      "cat.13.jpg\n",
      "cat.187.jpg\n",
      "cat.193.jpg\n",
      "cat.9.jpg\n",
      "cat.197.jpg\n",
      "cat.183.jpg\n",
      "cat.154.jpg\n",
      "cat.140.jpg\n",
      "cat.168.jpg\n",
      "cat.17.jpg\n",
      "cat.16.jpg\n",
      "cat.169.jpg\n",
      "cat.141.jpg\n",
      "cat.155.jpg\n",
      "cat.182.jpg\n",
      "cat.196.jpg\n",
      "cat.8.jpg\n",
      "cat.180.jpg\n",
      "cat.194.jpg\n",
      "cat.28.jpg\n",
      "cat.143.jpg\n",
      "cat.157.jpg\n",
      "cat.14.jpg\n",
      "cat.15.jpg\n",
      "cat.156.jpg\n",
      "cat.142.jpg\n",
      "cat.29.jpg\n",
      "cat.195.jpg\n",
      "cat.181.jpg\n",
      "Loaded the images of dataset-Humans\n",
      "\n",
      "rider-39.jpg\n",
      "rider-11.jpg\n",
      "rider-127.jpg\n",
      "rider-133.jpg\n",
      "rider-132.jpg\n",
      "rider-126.jpg\n",
      "rider-10.jpg\n",
      "rider-38.jpg\n",
      "rider-12.jpg\n",
      "rider-130.jpg\n",
      "rider-124.jpg\n",
      "rider-118.jpg\n",
      "rider-119.jpg\n",
      "rider-125.jpg\n",
      "rider-131.jpg\n",
      "rider-13.jpg\n",
      "rider-17.jpg\n",
      "rider-109.jpg\n",
      "rider-135.jpg\n",
      "rider-121.jpg\n",
      "rider-120.jpg\n",
      "rider-134.jpg\n",
      "rider-108.jpg\n",
      "rider-16.jpg\n",
      "rider-9.jpg\n",
      "rider-14.jpg\n",
      "rider-28.jpg\n",
      "rider-122.jpg\n",
      "rider-136.jpg\n",
      "rider-137.jpg\n",
      "rider-123.jpg\n",
      "rider-29.jpg\n",
      "rider-15.jpg\n",
      "rider-8.jpg\n",
      "rider-187.jpg\n",
      "rider-193.jpg\n",
      "rider-72.jpg\n",
      "rider-66.jpg\n",
      "rider-144.jpg\n",
      "rider-150.jpg\n",
      "rider-99.jpg\n",
      "rider-178.jpg\n",
      "rider-179.jpg\n",
      "rider-98.jpg\n",
      "rider-151.jpg\n",
      "rider-145.jpg\n",
      "rider-67.jpg\n",
      "rider-73.jpg\n",
      "rider-192.jpg\n",
      "rider-186.jpg\n",
      "rider-59.jpg\n",
      "rider-190.jpg\n",
      "rider-184.jpg\n",
      "rider-65.jpg\n",
      "rider-71.jpg\n",
      "rider-153.jpg\n",
      "rider-147.jpg\n",
      "rider-146.jpg\n",
      "rider-152.jpg\n",
      "rider-70.jpg\n",
      "rider-64.jpg\n",
      "rider-185.jpg\n",
      "rider-191.jpg\n",
      "rider-58.jpg\n",
      "rider-60.jpg\n",
      "rider-74.jpg\n",
      "rider-195.jpg\n",
      "rider-48.jpg\n",
      "rider-181.jpg\n",
      "rider-156.jpg\n",
      "rider-142.jpg\n",
      "rider-143.jpg\n",
      "rider-157.jpg\n",
      "rider-180.jpg\n",
      "rider-49.jpg\n",
      "rider-194.jpg\n",
      "rider-75.jpg\n",
      "rider-61.jpg\n",
      "rider-77.jpg\n",
      "rider-63.jpg\n",
      "rider-182.jpg\n",
      "rider-196.jpg\n",
      "rider-169.jpg\n",
      "rider-141.jpg\n",
      "rider-88.jpg\n",
      "rider-155.jpg\n",
      "rider-154.jpg\n",
      "rider-89.jpg\n",
      "rider-140.jpg\n",
      "rider-168.jpg\n",
      "rider-197.jpg\n",
      "rider-183.jpg\n",
      "rider-62.jpg\n",
      "rider-76.jpg\n",
      "rider-53.jpg\n",
      "rider-47.jpg\n",
      "rider-165.jpg\n",
      "rider-171.jpg\n",
      "rider-159.jpg\n",
      "rider-90.jpg\n",
      "rider-84.jpg\n",
      "rider-85.jpg\n",
      "rider-91.jpg\n",
      "rider-158.jpg\n",
      "rider-170.jpg\n",
      "rider-164.jpg\n",
      "rider-46.jpg\n",
      "rider-52.jpg\n",
      "rider-78.jpg\n",
      "rider-44.jpg\n",
      "rider-50.jpg\n",
      "rider-199.jpg\n",
      "rider-172.jpg\n",
      "rider-166.jpg\n",
      "rider-87.jpg\n",
      "rider-93.jpg\n",
      "rider-92.jpg\n",
      "rider-86.jpg\n",
      "rider-167.jpg\n",
      "rider-173.jpg\n",
      "rider-198.jpg\n",
      "rider-51.jpg\n",
      "rider-45.jpg\n",
      "rider-79.jpg\n",
      "rider-201.jpg\n",
      "rider-41.jpg\n",
      "rider-188.jpg\n",
      "rider-55.jpg\n",
      "rider-69.jpg\n",
      "rider-82.jpg\n",
      "rider-96.jpg\n",
      "rider-177.jpg\n",
      "rider-163.jpg\n",
      "rider-162.jpg\n",
      "rider-176.jpg\n",
      "rider-97.jpg\n",
      "rider-83.jpg\n",
      "rider-68.jpg\n",
      "rider-54.jpg\n",
      "rider-189.jpg\n",
      "rider-40.jpg\n",
      "rider-200.jpg\n",
      "rider-202.jpg\n",
      "rider-56.jpg\n",
      "rider-42.jpg\n",
      "rider-95.jpg\n",
      "rider-148.jpg\n",
      "rider-81.jpg\n",
      "rider-160.jpg\n",
      "rider-174.jpg\n",
      "rider-175.jpg\n",
      "rider-161.jpg\n",
      "rider-80.jpg\n",
      "rider-149.jpg\n",
      "rider-94.jpg\n",
      "rider-43.jpg\n",
      "rider-57.jpg\n",
      "rider-5.jpg\n",
      "rider-18.jpg\n",
      "rider-30.jpg\n",
      "rider-24.jpg\n",
      "rider-106.jpg\n",
      "rider-112.jpg\n",
      "rider-113.jpg\n",
      "rider-107.jpg\n",
      "rider-25.jpg\n",
      "rider-31.jpg\n",
      "rider-19.jpg\n",
      "rider-4.jpg\n",
      "rider-6.jpg\n",
      "rider-27.jpg\n",
      "rider-33.jpg\n",
      "rider-111.jpg\n",
      "rider-105.jpg\n",
      "rider-139.jpg\n",
      "rider-138.jpg\n",
      "rider-104.jpg\n",
      "rider-110.jpg\n",
      "rider-32.jpg\n",
      "rider-26.jpg\n",
      "rider-7.jpg\n",
      "rider-3.jpg\n",
      "rider-22.jpg\n",
      "rider-36.jpg\n",
      "rider-128.jpg\n",
      "rider-114.jpg\n",
      "rider-100.jpg\n",
      "rider-101.jpg\n",
      "rider-115.jpg\n",
      "rider-129.jpg\n",
      "rider-37.jpg\n",
      "rider-23.jpg\n",
      "rider-2.jpg\n",
      "rider-35.jpg\n",
      "rider-21.jpg\n",
      "rider-103.jpg\n",
      "rider-117.jpg\n",
      "rider-116.jpg\n",
      "rider-102.jpg\n",
      "rider-20.jpg\n",
      "rider-34.jpg\n",
      "rider-1.jpg\n",
      "Loaded the images of dataset-horses\n",
      "\n",
      "horse-138.jpg\n",
      "horse-104.jpg\n",
      "horse-110.jpg\n",
      "horse-26.jpg\n",
      "horse-32.jpg\n",
      "horse-33.jpg\n",
      "horse-27.jpg\n",
      "horse-111.jpg\n",
      "horse-105.jpg\n",
      "horse-139.jpg\n",
      "horse-113.jpg\n",
      "horse-107.jpg\n",
      "horse-31.jpg\n",
      "horse-25.jpg\n",
      "horse-19.jpg\n",
      "horse-18.jpg\n",
      "horse-24.jpg\n",
      "horse-30.jpg\n",
      "horse-106.jpg\n",
      "horse-112.jpg\n",
      "horse-116.jpg\n",
      "horse-102.jpg\n",
      "horse-34.jpg\n",
      "horse-20.jpg\n",
      "horse-21.jpg\n",
      "horse-35.jpg\n",
      "horse-103.jpg\n",
      "horse-117.jpg\n",
      "horse-101.jpg\n",
      "horse-115.jpg\n",
      "horse-129.jpg\n",
      "horse-23.jpg\n",
      "horse-37.jpg\n",
      "horse-36.jpg\n",
      "horse-22.jpg\n",
      "horse-128.jpg\n",
      "horse-114.jpg\n",
      "horse-100.jpg\n",
      "horse-198.jpg\n",
      "horse-167.jpg\n",
      "horse-173.jpg\n",
      "horse-86.jpg\n",
      "horse-92.jpg\n",
      "horse-45.jpg\n",
      "horse-51.jpg\n",
      "horse-79.jpg\n",
      "horse-78.jpg\n",
      "horse-50.jpg\n",
      "horse-44.jpg\n",
      "horse-93.jpg\n",
      "horse-87.jpg\n",
      "horse-172.jpg\n",
      "horse-166.jpg\n",
      "horse-199.jpg\n",
      "horse-9.jpg\n",
      "horse-158.jpg\n",
      "horse-170.jpg\n",
      "horse-164.jpg\n",
      "horse-91.jpg\n",
      "horse-85.jpg\n",
      "horse-52.jpg\n",
      "horse-46.jpg\n",
      "horse-47.jpg\n",
      "horse-53.jpg\n",
      "horse-84.jpg\n",
      "horse-90.jpg\n",
      "horse-165.jpg\n",
      "horse-171.jpg\n",
      "horse-159.jpg\n",
      "horse-8.jpg\n",
      "horse-175.jpg\n",
      "horse-161.jpg\n",
      "horse-149.jpg\n",
      "horse-94.jpg\n",
      "horse-80.jpg\n",
      "horse-57.jpg\n",
      "horse-43.jpg\n",
      "horse-42.jpg\n",
      "horse-56.jpg\n",
      "horse-81.jpg\n",
      "horse-95.jpg\n",
      "horse-148.jpg\n",
      "horse-160.jpg\n",
      "horse-174.jpg\n",
      "horse-202.jpg\n",
      "horse-200.jpg\n",
      "horse-189.jpg\n",
      "horse-162.jpg\n",
      "horse-176.jpg\n",
      "horse-83.jpg\n",
      "horse-97.jpg\n",
      "horse-68.jpg\n",
      "horse-40.jpg\n",
      "horse-54.jpg\n",
      "horse-55.jpg\n",
      "horse-41.jpg\n",
      "horse-69.jpg\n",
      "horse-96.jpg\n",
      "horse-82.jpg\n",
      "horse-177.jpg\n",
      "horse-163.jpg\n",
      "horse-188.jpg\n",
      "horse-201.jpg\n",
      "horse-3.jpg\n",
      "horse-185.jpg\n",
      "horse-191.jpg\n",
      "horse-146.jpg\n",
      "horse-152.jpg\n",
      "horse-64.jpg\n",
      "horse-70.jpg\n",
      "horse-58.jpg\n",
      "horse-59.jpg\n",
      "horse-71.jpg\n",
      "horse-65.jpg\n",
      "horse-153.jpg\n",
      "horse-147.jpg\n",
      "horse-190.jpg\n",
      "horse-184.jpg\n",
      "horse-2.jpg\n",
      "horse-192.jpg\n",
      "horse-186.jpg\n",
      "horse-179.jpg\n",
      "horse-151.jpg\n",
      "horse-145.jpg\n",
      "horse-98.jpg\n",
      "horse-73.jpg\n",
      "horse-67.jpg\n",
      "horse-66.jpg\n",
      "horse-72.jpg\n",
      "horse-99.jpg\n",
      "horse-144.jpg\n",
      "horse-150.jpg\n",
      "horse-178.jpg\n",
      "horse-187.jpg\n",
      "horse-193.jpg\n",
      "horse-1.jpg\n",
      "horse-5.jpg\n",
      "horse-197.jpg\n",
      "horse-183.jpg\n",
      "horse-154.jpg\n",
      "horse-140.jpg\n",
      "horse-168.jpg\n",
      "horse-89.jpg\n",
      "horse-76.jpg\n",
      "horse-62.jpg\n",
      "horse-63.jpg\n",
      "horse-77.jpg\n",
      "horse-88.jpg\n",
      "horse-169.jpg\n",
      "horse-141.jpg\n",
      "horse-155.jpg\n",
      "horse-182.jpg\n",
      "horse-196.jpg\n",
      "horse-4.jpg\n",
      "horse-6.jpg\n",
      "horse-180.jpg\n",
      "horse-194.jpg\n",
      "horse-143.jpg\n",
      "horse-157.jpg\n",
      "horse-49.jpg\n",
      "horse-61.jpg\n",
      "horse-75.jpg\n",
      "horse-74.jpg\n",
      "horse-60.jpg\n",
      "horse-48.jpg\n",
      "horse-156.jpg\n",
      "horse-142.jpg\n",
      "horse-195.jpg\n",
      "horse-181.jpg\n",
      "horse-7.jpg\n",
      "horse-119.jpg\n",
      "horse-125.jpg\n",
      "horse-131.jpg\n",
      "horse-13.jpg\n",
      "horse-12.jpg\n",
      "horse-130.jpg\n",
      "horse-124.jpg\n",
      "horse-118.jpg\n",
      "horse-132.jpg\n",
      "horse-126.jpg\n",
      "horse-10.jpg\n",
      "horse-38.jpg\n",
      "horse-39.jpg\n",
      "horse-11.jpg\n",
      "horse-127.jpg\n",
      "horse-133.jpg\n",
      "horse-137.jpg\n",
      "horse-123.jpg\n",
      "horse-29.jpg\n",
      "horse-15.jpg\n",
      "horse-14.jpg\n",
      "horse-28.jpg\n",
      "horse-122.jpg\n",
      "horse-136.jpg\n",
      "horse-120.jpg\n",
      "horse-134.jpg\n",
      "horse-108.jpg\n",
      "horse-16.jpg\n",
      "horse-17.jpg\n",
      "horse-109.jpg\n",
      "horse-135.jpg\n",
      "horse-121.jpg\n",
      "(808, 16384)\n",
      "-6.528479e-09\n",
      "0.9999998\n",
      "[-2.2602553e-07 -7.3325516e-08  1.2555334e-07 ...  1.2293458e-07\n",
      " -1.6192104e-08  4.9424642e-09]\n",
      "[0.9999991  0.9999998  0.99999964 ... 0.9999995  1.0000006  0.999999  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgalembeck/miniforge3/envs/IA/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:239: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/Users/dgalembeck/miniforge3/envs/IA/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:258: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "img_data_list=[]\n",
    "\n",
    "for dataset in data_dir_list: # boucle sur les 4 repertoires\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)  # \n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        print(img)\n",
    "        input_img_raw = cv2.imread(\"/Users/dgalembeck/Documents/Coding/Cours/STASC/Data/data_animals/\" + dataset + '/' + img) \n",
    "        input_img_grey= cv2.cvtColor(input_img_raw, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_flatten=cv2.resize(input_img_grey,(128,128)).flatten()\n",
    "        img_data_list.append(input_img_flatten)\n",
    "\n",
    "        \n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "\n",
    "img_data_scaled = preprocessing.scale(img_data)\n",
    "print (img_data_scaled.shape)\n",
    "\n",
    "print (np.mean(img_data_scaled))\n",
    "print (np.std(img_data_scaled))\n",
    "\n",
    "print (img_data_scaled.mean(axis=0))\n",
    "print (img_data_scaled.std(axis=0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quelle la dimension du tableau `img_data` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808, 16384)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data.shape # 808 images with 128*128 features each (pixels in grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De façon générale, la première couche du réseau de convolution  prend en entrée un objet de dimension 3 : hauteur, largeur, profondeur,  où la profondeur correspond aux nombres de canaux.\n",
    "\n",
    "Avec Tensor Flow (ici en backend) la profondeur doit être donnée en dernière position.\n",
    "\n",
    "Cette dimension est ici \"factice\"  car nos images sont en niveaux gris, elle est néanmoins nécessaire car attendue par les fonctions de Keras et Tensor Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'échantillon d'images doit finalement se présenter sous la forme d'un objet de dimension 4: (nombre d'échantillons, hauteur, largeur, profondeur)\n",
    "\n",
    "Nous redimensionnons les données pour qu'elle se présente ainsi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "img_data_reshape=img_data_scaled.reshape(img_data.shape[0],\n",
    "                                        img_rows,img_cols,\n",
    "                                        num_channel)\n",
    "print (img_data_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimension d'une image en entrée du réseau est la suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=img_data_reshape[0].shape # (128, 128, 1)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous indiquons maintenant les labels des images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = img_data_reshape.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "labels[0:202]=0\n",
    "labels[202:404]=1\n",
    "labels[404:606]=2\n",
    "labels[606:]=3\n",
    "\n",
    "names = ['cats','dogs','horses','humans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Convertir les labels en \"one-hot encoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = to_categorical(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Séparer  aléatoirement les données en un échantillon d'apprentissage (80%) et un échantillon de test (20%). Assurez-vous que les données d'apprentissage prennent bien la forme d'un tableau de dimension 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_data, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de l'architecture du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons ci-dessous les deux premiers niveaux de convolution du réseau CNN.\n",
    "Chacune de ces deux couches est définie comme suit :\n",
    "+ 32 noyaux (filtres)\n",
    "+ Pas (stride) = 1\n",
    "+ Kernel size = (3,3)\n",
    "+ padding = 'same' (i.e. 0 padding : bordures à 0)\n",
    "+ activation : relu\n",
    "\n",
    "> Créer un modèle séquentiel que vous nommerez `my_first_CNN` composé de 4 couches succesives (conv + relu + conv + relu).    \n",
    "> Voir  [ici](https://keras.io/layers/convolutional/#conv2d) et  [ici](https://keras.io/examples/vision/mnist_convnet/) pour la synthaxe de la couche de convolution `Conv2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 11:55:32.399689: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-16 11:55:32.401267: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 57600)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 57600)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 230404    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 249,220\n",
      "Trainable params: 249,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "my_first_CNN = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        #layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        #layers.Dropout(0.5),\n",
    "        #layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_first_CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Executez les codes ci-dessous et decrire les sorties obtenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_first_CNN.layers[0].input_shape)\n",
    "print(my_first_CNN.layers[1].input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`batch_size`,`n_l`,`n_c`,`nb de canaux`)\n",
    "\n",
    "Keras dimensionnera ensuite correctement les couches en fonction du `batch_size` choisi par l'utilisateur. \n",
    "\n",
    "> Passer `batch_size=16` en argument de `Conv2D` et vérifier que cela a bien été pris en compte dans les dimensions de la couche cachée de `my_first_CNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Executez les codes ci-dessous et decrire les sorties obtenues. Expliquer en particulier la dimension de la troisième couche en utilisant `get_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(my_first_CNN.layers[2].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multichannel convolution:\n",
    "\\begin{eqnarray} Z(i,j,l) &= & ( V \\star K ) (i,j,l) \\\\\n",
    "& =  & \\sum_{u,v,w}   V (i+u, j+v,w ) w_{u,v,l,w} \n",
    "\\end{eqnarray}\n",
    "where \n",
    "+ $V$  and $Z$ have the same dimensions (multichannel).\n",
    "+ $K(u,v,l,w) $ gives the connection strength between a unit in channel $l$ of the output and a unit in channel $w$ of the input, with an offset of  $u$ rows and $v$ columns between the output unit and the input unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de poids à estimer vaut donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de paramètres à estimer pour les termes de biais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien que pour la troisième couche 9248 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que les poids sont (déjà) initialisés aléatoirement alors que les biais sont initialisés à 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Construire maintenant l'architecture complète du réseau `my_first_CNN` :\n",
    "+ Convolution à 32 filtres de taille (3,3), zero padding\n",
    "+ Activation Relu\n",
    "+ Convolution à 32 filtres de taille (3,3), zero padding\n",
    "+ Activation Relu\n",
    "+ Maxpooling2D (2,2) [documentation](https://keras.io/api/layers/pooling_layers/max_pooling2d/)\n",
    "+ Dropout(0.5) [documentation](https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "+ Convolution à 64 filtres de taille (3,3), zero padding\n",
    "+ Maxpooling2D (2,2) \n",
    "+ Dropout(0.5) \n",
    "+ Flatten  [documentation](https://keras.io/api/layers/reshaping_layers/flatten/)\n",
    "+ Dense(64)\n",
    "+ Activation Relu\n",
    "+ Dropout(0.5)\n",
    "+ Dense(4)\n",
    "+ Softmax   \n",
    "> \n",
    "> Afficher un résumé de l'architecture avec `my_first_CNN.summary` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ajuster le modèle \n",
    "- avec la méthode sgd (avec un taux d'apprentissage de 0.01 et momentum de 0.9)\n",
    "- puis la méthode adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tracer en fonction du nombre d'epochs le risque de cross-entropy ainsi que la précision pour les échantillons d'apprentissage et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Donner le risque de cross-entropy ainsi que la précision pour l'échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utilisez les outils `classification_report()` et `confusion_matrix()` de `sklearn.metrics` pour décrire les performances du réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour afficher la matrice de confusion sous forme graphique, on dispose de la fonction [`plot_confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) de sklearn. Pour pouvoir utiliser les fonctionnalités de sklearn, il nous faut tout d'abord transformer l'objet Keras en un classifieur sklearn. On peut pour cela utiliser la fonction wrapper [`KerasClassifier`](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasClassifier) du module `keras.wrappers.scikit_learn` (voir aussi le TP précédent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `KerasClassifier` est le plus souvent utilisée pour ajuster un réseau, typiquement pour une procédure de type validation croisée (`Gridsearch`). Ici, au contraire, on ne souhaite pas réajuster une nouvelle fois le modèle, mais uniquement changer sa forme. \n",
    "\n",
    "> Compléter le code ci-dessous pour créer l'objet `wrapped_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = KerasClassifier(build_fn = lambda : ### TO DO ###,\n",
    "                                epochs = ### TO DO ###) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ajuster ce modèle sur les données d'apprentissage. Assurez vous que les prédictions de `wrapped_model` et de `my_first_CNN` sur les données de test sont bien identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Essayer maintenant d'appliquer la fonction `plot_confusion_matrix` au modèle `wrapped_model`et aux données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela ne fonctionne pas, il y a en effet un petit bug dans la fonction `KerasClassifier`. En étudiant l'erreur renvoyée ci-dessus, on comprend que la fonction `plot_confusion_matrix` teste si `wrapped_model` est un classifieur, et que le test ici ne passe pas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import is_classifier\n",
    "is_classifier(wrapped_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème vient du fait que wrapped_model ne possède pas d'attribut \"_estimator_type\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model._estimator_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utiliser la fonction [`setattr`](https://docs.python.org/3/library/functions.html#setattr) pour résoudre ce problème et applique finalement la fonction `plot_confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde d'un réseau de neurones avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'un modèle a été ajusté, on peut vouloir conserver \n",
    "- l'architecture du réseau\n",
    "- la valeurs des poids des couches\n",
    "- l'optimiseur utilisé pour ajuster les poids \n",
    "- les métriques et les pertes considérées \n",
    "\n",
    "Pour répondre aux questions ci-dessous, vous pourrez consulter cette [page](https://keras.io/guides/serialization_and_saving/) de la documentation qui présente en détail les méthodes pour sauvegarder des réseaux Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sauvegarde du modèle (architecture seule) en json\n",
    "\n",
    "Dans certaines situations, on ne souhaite sauvegarder que l'architecture. Par exemple si on veut comparer plusieurs méthodes d'optimisation des poids d'un même réseau. Il est possible de sauvegarder l'architecture d'un réseau au format JSON.\n",
    "\n",
    "JavaScript Object Notation (JSON) est un format de données textuelles dérivé de la notation des objets du langage JavaScript. Il permet de représenter de l’information structurée.\n",
    "\n",
    "Un document JSON a pour fonction de représenter de l'information accompagnée d'étiquettes permettant d'en interpréter les divers éléments, sans aucune restriction sur le nombre de celles-ci.\n",
    "\n",
    "Un document JSON ne comprend que deux types d'éléments structurels :\n",
    "+ Des ensembles de paires \"nom\" (alias \"clé\") / \"valeur\" ;\n",
    "+ Des listes ordonnées de valeurs.\n",
    "\n",
    "> Sauver l'architecture du réseau au format json.   \n",
    "> Afficher le contenu du fichier sauvé.   \n",
    "> Quelle est la taille du fichier json sur votre disque ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sauvegarde et chargement du modèle complet entrainé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utiliser les fonctions `model.save()` et `load_model()` pour sauver et charger un modèle complet (architecture, poids, optimiseur, métriques). Quelle est la taille du répertoire créé pour cette sauvegarde ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparer les poids du réseau reconstruit aux poids du réseau originel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que le modèle chargé peut être directement utilisé pour faire des prédictions ou pour calculer un score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : ajustement du modèle sur Google Colab\n",
    "\n",
    "> Ajuster ce modèle CNN (ou évenuellement un modèle plus profond) sur [Google Colab](https://colab.research.google.com/notebooks) (ou sur [Binder](https://mybinder.org/)). \n",
    "> Il vous faudra telecharger les données sur Colab et adapter les codes du TP pour l'importation des images, plusieurs solutions sont possibles, voir par exemple\n",
    "[ici](https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "87fc21197254558951455343ce704bc14f1a5282bb938462d86fd102e291cdae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
